{
    "metadata": {
        "language_info": {
            "file_extension": ".py", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "version": "2.7.11", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "pygments_lexer": "ipython2", 
            "name": "python"
        }, 
        "kernelspec": {
            "name": "python2-spark21", 
            "language": "python", 
            "display_name": "Python 2 with Spark 2.1"
        }
    }, 
    "nbformat": 4, 
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Python DL_NLP Crash Course_Part 7: Neural Machine Translation\n\n## Full Day Workshop for user learn Data Science with Python\n### 2018 Timothy CL Lam\nThis is meant for internal usage, part of contents copied externally, not for commercial purpose\n", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Encoder-Decoder Models for Neural Machine Translation\n- The encoder-decoder architecture for recurrent neural networks is the standard neural machine\ntranslation method that rivals and in some cases outperforms classical statistical machine\ntranslation methods. \n- This architecture is very new, having only been pioneered in 2014,\nalthough, has been adopted as the core technology inside Google's translate service. \n\n## Encoder-Decoder Architecture for NMT\n- The Encoder-Decoder architecture with recurrent neural networks has become an e\u000bective\nand standard approach \n- for both neural machine translation (NMT) and sequence-to-sequence\n(seq2seq) prediction in general.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "# German to English Translation Dataset\nIn this tutorial, we will use a dataset of German to English terms used as the basis for \nashcards\nfor language learning. The dataset is available from the ManyThings.org website, with examples\ndrawn from the Tatoeba Project.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "source": "# load doc into memory\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename, mode='rt', encoding='utf-8')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "source": "# split a loaded document into sentences\ndef to_pairs(doc):\n    lines = doc.strip().split('\\n')\n    pairs = [line.split('\\t') for line in lines]\n    return pairs", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We are now ready to clean each sentence. The speci\fc cleaning operations we will perform\nare as follows:\n- Remove all non-printable characters.\n- Remove all punctuation characters.\n- Normalize all Unicode characters to ASCII (e.g. Latin characters).\n- Normalize the case to lowercase.\n- Remove any remaining tokens that are not alphabetic.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 2, 
            "source": "# clean a list of lines\ndef clean_pairs(lines):\n    cleaned = list()\n    # prepare regex for char filtering\n    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n    re_print = re.compile('[^%s]' % re.escape(string.printable))\n    for pair in lines:\n        clean_pair = list()\n        for line in pair:\n            # normalize unicode characters\n            line = normalize('NFD', line).encode('ascii', 'ignore')\n            line = line.decode('UTF-8')\n            # tokenize on white space\n            line = line.split()\n            # convert to lowercase\n            line = [word.lower() for word in line]\n            # remove punctuation from each token\n            line = [re_punc.sub('', w) for w in line]\n            # remove non-printable chars form each token\n            line = [re_print.sub('', w) for w in line]\n            # remove tokens with numbers in them\n            line = [word for word in line if word.isalpha()]\n            # store as string\n            clean_pair.append(' '.join(line))\n        cleaned.append(clean_pair)\n    return array(cleaned)", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "- Finally, now that the data has been cleaned, we can save the list of phrase pairs to a \fle\nready for use. \n- The function save clean data() uses the pickle API to save the list of clean\ntext to \fle. Pulling all of this together, the complete example is listed below.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "source": "import string\nimport re\nfrom pickle import dump\nfrom unicodedata import normalize\nfrom numpy import array\n\n# save a list of clean sentences to file\ndef save_clean_data(sentences, filename):\n    dump(sentences, open(filename, 'wb'))\n    print('Saved: %s' % filename)\n    \n    \n# load dataset\nfilename = 'deu.txt'\ndoc = load_doc(filename)\n# split into english-german pairs\npairs = to_pairs(doc)\n# clean sentences\nclean_pairs = clean_pairs(pairs)\n# save clean pairs to file\nsave_clean_data(clean_pairs, 'english-german.pkl')\n# spot check\nfor i in range(100):\n    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "- The clean data contains a little over 150,000 phrase pairs and some of the pairs toward the end\nof the \fle are very long. \n- This is a good number of examples for developing a small translation\nmodel.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "source": "from pickle import load\nfrom pickle import dump\nfrom numpy.random import shuffle\n\n# load a clean dataset\ndef load_clean_sentences(filename):\n    return load(open(filename, 'rb'))\n# save a list of clean sentences to file\n\ndef save_clean_data(sentences, filename):\n    dump(sentences, open(filename, 'wb'))\n    print('Saved: %s' % filename)\n    \n    \n# load dataset\nraw_dataset = load_clean_sentences('english-german.pkl')\n\n\n# reduce dataset size\nn_sentences = 10000\ndataset = raw_dataset[:n_sentences, :]\n# random shuffle\nshuffle(dataset)\n# split into train/test\ntrain, test = dataset[:9000], dataset[9000:]\n\n\n# save\nsave_clean_data(dataset, 'english-german-both.pkl')\nsave_clean_data(train, 'english-german-train.pkl')\nsave_clean_data(test, 'english-german-test.pkl')", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "- the english-german-both.pkl that contains\nall of the train and test examples that \n- we can use to define the parameters of the problem,\nsuch as max phrase lengths and the vocabulary, \n- and the english-german-train.pkl and\nenglish-german-test.pkl \fles for the train and test dataset.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": " # Train Neural Translation Model\nIn this section, we will develop the translation model. This involves both loading and preparing\nthe clean text data ready for modeling and de\fning and training the model on the prepared\ndata.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 3, 
            "source": "from pickle import load\nfrom numpy import array\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Embedding\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.callbacks import ModelCheckpoint\n\n\n# load a clean dataset\ndef load_clean_sentences(filename):\n    return load(open(filename, 'rb'))\n\n\n# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n\n# max sentence length\ndef max_length(lines):\n    return max(len(line.split()) for line in lines)\n\n\n# encode and pad sequences\ndef encode_sequences(tokenizer, length, lines):\n    # integer encode sequences\n    X = tokenizer.texts_to_sequences(lines)\n    # pad sequences with 0 values\n    X = pad_sequences(X, maxlen=length, padding='post')\n    return X\n\n# one hot encode target sequence\ndef encode_output(sequences, vocab_size):\n    ylist = list()\n    for sequence in sequences:\n        encoded = to_categorical(sequence, num_classes=vocab_size)\n        ylist.append(encoded)\n    y = array(ylist)\n    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n    return y\n\n\n# define NMT model\ndef define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n    model = Sequential()\n    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n    model.add(LSTM(n_units))\n    model.add(RepeatVector(tar_timesteps))\n    model.add(LSTM(n_units, return_sequences=True))\n    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n    # compile model\n    model.compile(optimizer='adam', loss='categorical_crossentropy')\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model\n\n\n# load datasets\ndataset = load_clean_sentences('english-german-both.pkl')\ntrain = load_clean_sentences('english-german-train.pkl')\ntest = load_clean_sentences('english-german-test.pkl')\n\n\n# prepare english tokenizer\neng_tokenizer = create_tokenizer(dataset[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\neng_length = max_length(dataset[:, 0])\nprint('English Vocabulary Size: %d' % eng_vocab_size)\nprint('English Max Length: %d' % (eng_length))\n\n\n# prepare german tokenizer\nger_tokenizer = create_tokenizer(dataset[:, 1])\nger_vocab_size = len(ger_tokenizer.word_index) + 1\nger_length = max_length(dataset[:, 1])\nprint('German Vocabulary Size: %d' % ger_vocab_size)\nprint('German Max Length: %d' % (ger_length))\n\n\n# prepare training data\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\ntrainY = encode_output(trainY, eng_vocab_size)\n\n\n# prepare validation data\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\ntestY = encode_output(testY, eng_vocab_size)\n\n\n# define model\nmodel = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n\n# fit model\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1,\n    save_best_only=True, mode='min')\nmodel.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY),\ncallbacks=[checkpoint], verbose=2)", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "/gpfs/fs01/user/sbb6-28ae32a56257b0-666d72869b6d/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "## Evaluate Neural Translation Model\nWe will evaluate the model on the train and the test dataset. The model should perform very\nwell on the train dataset and ideally have been generalized to perform well on the test dataset.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "source": "from pickle import load\nfrom numpy import argmax\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom nltk.translate.bleu_score import corpus_bleu\n\n\n# load a clean dataset\ndef load_clean_sentences(filename):\n    return load(open(filename, 'rb'))\n\n\n# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n\n# max sentence length\ndef max_length(lines):\n    return max(len(line.split()) for line in lines)\n\n\n# encode and pad sequences\ndef encode_sequences(tokenizer, length, lines):\n    # integer encode sequences\n    X = tokenizer.texts_to_sequences(lines)\n    # pad sequences with 0 values\n    X = pad_sequences(X, maxlen=length, padding='post')\n    return X\n\n\n# map an integer to a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\n\n# generate target given source sequence\ndef predict_sequence(model, tokenizer, source):\n    prediction = model.predict(source, verbose=0)[0]\n    integers = [argmax(vector) for vector in prediction]\n    target = list()\n    for i in integers:\n        word = word_for_id(i, tokenizer)\n        if word is None:\n            break\n        target.append(word)\n    return ' '.join(target)\n\n\n# evaluate the skill of the model\ndef evaluate_model(model, sources, raw_dataset):\n    actual, predicted = list(), list()\n    for i, source in enumerate(sources):\n        # translate encoded source text\n        source = source.reshape((1, source.shape[0]))\n        translation = predict_sequence(model, eng_tokenizer, source)\n        raw_target, raw_src = raw_dataset[i]\n        if i < 10:\n            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n        actual.append(raw_target.split())\n        predicted.append(translation.split())\n        # calculate BLEU score\n        print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n        print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n        print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n        print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n        \n        \n# load datasets\ndataset = load_clean_sentences('english-german-both.pkl')\ntrain = load_clean_sentences('english-german-train.pkl')\ntest = load_clean_sentences('english-german-test.pkl')\n\n\n# prepare english tokenizer\neng_tokenizer = create_tokenizer(dataset[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\neng_length = max_length(dataset[:, 0])\n\n\n# prepare german tokenizer\nger_tokenizer = create_tokenizer(dataset[:, 1])\nger_vocab_size = len(ger_tokenizer.word_index) + 1\nger_length = max_length(dataset[:, 1])\n\n\n# prepare data\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n\n\n# load model\nmodel = load_model('model.h5')\n\n\n# test on some training sequences\nprint('train')\nevaluate_model(model, trainX, train)\n\n\n# test on some test sequences\nprint('test')\nevaluate_model(model, testX, test)", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code"
        }
    ]
}